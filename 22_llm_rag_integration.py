"""
Phase 7, Cell 5: LLM Integration with RAG Pipeline
This cell integrates the fine-tuned LLM with the existing RAG pipeline
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
import os
from datetime import datetime

print("üîó Integrating Fine-tuned LLM with RAG Pipeline...")

# Load the fine-tuned model
model_path = os.path.join(LLM_DIR, 'agricultural_llm')
model_info_path = os.path.join(LLM_DIR, 'model_info.json')

if os.path.exists(model_path) and os.path.exists(model_info_path):
    print("‚úÖ Fine-tuned model found!")
    
    # Load model info
    with open(model_info_path, 'r') as f:
        model_info = json.load(f)
    
    print(f"Model: {model_info['model_name']}")
    print(f"Training samples: {model_info['training_samples']:,}")
    print(f"Fine-tuned: {model_info['fine_tuned']}")
    
    # Load the fine-tuned model and tokenizer
    try:
        local_tokenizer = AutoTokenizer.from_pretrained(model_path)
        local_model = AutoModelForCausalLM.from_pretrained(model_path)
        
        print("‚úÖ Fine-tuned model loaded successfully!")
        
        # Test the fine-tuned model
        print("\nüß™ Testing Fine-tuned Model...")
        
        test_prompts = [
            "Agricultural Recommendation: Soil pH 6.2, loamy texture, temperature 24¬∞C, rainfall 750mm. I recommend",
            "What crops are suitable for loamy soil with pH 6.5?",
            "How much rainfall does maize need for optimal growth?",
            "Based on agricultural research, cassava requires"
        ]
        
        for i, prompt in enumerate(test_prompts):
            print(f"\n--- Test {i+1} ---")
            print(f"Prompt: {prompt}")
            
            inputs = local_tokenizer.encode(prompt, return_tensors='pt')
            
            with torch.no_grad():
                outputs = local_model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 150,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=local_tokenizer.eos_token_id,
                    eos_token_id=local_tokenizer.eos_token_id
                )
            
            response = local_tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"Response: {response}")
        
        # Create integration class
        class FineTunedAgriculturalLLM:
            """Fine-tuned LLM for agricultural recommendations"""
            
            def __init__(self, model_path):
                self.model_path = model_path
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                self.model = AutoModelForCausalLM.from_pretrained(model_path)
                self.model.eval()
                
            def generate_recommendation(self, context, max_length=200):
                """Generate agricultural recommendation based on context"""
                
                # Create prompt
                prompt = f"""Agricultural Recommendation:

Context: {context}

Recommendation:"""
                
                inputs = self.tokenizer.encode(prompt, return_tensors='pt')
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + max_length,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # Extract just the recommendation part
                if "Recommendation:" in response:
                    recommendation = response.split("Recommendation:")[-1].strip()
                else:
                    recommendation = response
                
                return recommendation
            
            def answer_question(self, question, max_length=150):
                """Answer agricultural questions"""
                
                prompt = f"""Agricultural Question: {question}

Answer:"""
                
                inputs = self.tokenizer.encode(prompt, return_tensors='pt')
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        max_length=inputs.shape[1] + max_length,
                        num_return_sequences=1,
                        temperature=0.6,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                if "Answer:" in response:
                    answer = response.split("Answer:")[-1].strip()
                else:
                    answer = response
                
                return answer
        
        # Initialize the fine-tuned LLM
        fine_tuned_llm = FineTunedAgriculturalLLM(model_path)
        
        print("\n‚úÖ Fine-tuned LLM integration complete!")
        
        # Test integration
        print("\nüî¨ Testing Integration...")
        
        # Test recommendation generation
        test_context = "Soil pH: 6.2, Organic Matter: 2.1%, Soil Texture: loamy, Temperature: 24¬∞C, Rainfall: 750mm"
        recommendation = fine_tuned_llm.generate_recommendation(test_context)
        print(f"Generated recommendation: {recommendation}")
        
        # Test question answering
        test_question = "What are the optimal growing conditions for maize?"
        answer = fine_tuned_llm.answer_question(test_question)
        print(f"Generated answer: {answer}")
        
        # Save integration configuration
        integration_config = {
            'model_type': 'fine_tuned_llm',
            'model_path': model_path,
            'model_info': model_info,
            'integration_timestamp': datetime.now().isoformat(),
            'capabilities': [
                'agricultural_recommendations',
                'question_answering',
                'context_aware_responses'
            ]
        }
        
        config_path = os.path.join(LLM_DIR, 'integration_config.json')
        with open(config_path, 'w') as f:
            json.dump(integration_config, f, indent=2)
        
        print(f"‚úÖ Integration config saved to: {config_path}")
        
    except Exception as e:
        print(f"‚ùå Error loading fine-tuned model: {e}")
        print("Using base model instead...")
        
        # Fallback to base model
        local_tokenizer = tokenizer
        local_model = model
        
else:
    print("‚ùå Fine-tuned model not found!")
    print("Using base model for integration...")
    
    # Use base model
    local_tokenizer = tokenizer
    local_model = model

# Create deployment-ready integration
class AgriculturalLLMIntegration:
    """Deployment-ready LLM integration for agricultural recommendations"""
    
    def __init__(self, model, tokenizer, model_path=None):
        self.model = model
        self.tokenizer = tokenizer
        self.model_path = model_path
        self.is_fine_tuned = model_path is not None
        
    def generate_agricultural_response(self, context, response_type="recommendation"):
        """Generate agricultural response based on context"""
        
        if response_type == "recommendation":
            prompt = f"""Agricultural Recommendation:

Context: {context}

Recommendation:"""
        elif response_type == "question":
            prompt = f"""Agricultural Question: {context}

Answer:"""
        else:
            prompt = f"""Agricultural Information:

Context: {context}

Response:"""
        
        inputs = self.tokenizer.encode(prompt, return_tensors='pt')
        
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                max_length=inputs.shape[1] + 200,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract the relevant part
        if "Recommendation:" in response:
            return response.split("Recommendation:")[-1].strip()
        elif "Answer:" in response:
            return response.split("Answer:")[-1].strip()
        elif "Response:" in response:
            return response.split("Response:")[-1].strip()
        else:
            return response

# Initialize the integration
llm_integration = AgriculturalLLMIntegration(
    model=local_model,
    tokenizer=local_tokenizer,
    model_path=model_path if os.path.exists(model_path) else None
)

print(f"\nüéØ LLM Integration Status:")
print(f"Model type: {'Fine-tuned' if llm_integration.is_fine_tuned else 'Base'}")
print(f"Model parameters: {llm_integration.model.num_parameters():,}")
print(f"Ready for deployment: Yes")

# Final test
print("\nüß™ Final Integration Test...")
test_context = "Soil pH: 6.5, loamy soil, temperature 25¬∞C, rainfall 800mm"
response = llm_integration.generate_agricultural_response(test_context, "recommendation")
print(f"Final test response: {response}")

print("\n‚úÖ LLM Integration with RAG Pipeline Complete!")
print("The fine-tuned LLM is now ready for deployment in your agricultural recommendation system.")
